# Hydra configuration for model management
# Version: 1.0
# Contains configurations for model types, hyperparameters, training, evaluation,
# and hyperparameter tuning for the ERCOT RTLMP spike prediction system

# Default model settings
model_defaults:
  model_type: xgboost  # Default model type
  random_state: ${system.random_seed}  # Use system-wide random seed for reproducibility
  model_registry_path: ${paths.model_dir}  # Path to model registry
  version_format: "%Y%m%d_%H%M%S"  # Format for model version naming

# Configuration for different model types
model_types:
  # XGBoost configuration
  xgboost:
    hyperparameters:
      learning_rate: 0.05  # Step size shrinkage to prevent overfitting
      max_depth: 6  # Maximum depth of a tree
      min_child_weight: 1  # Minimum sum of instance weight needed in a child
      subsample: 0.8  # Subsample ratio of the training instances
      colsample_bytree: 0.8  # Subsample ratio of columns when constructing each tree
      n_estimators: 200  # Number of trees to fit
      objective: "binary:logistic"  # Objective function for binary classification
      eval_metric: "auc"  # Evaluation metric
      use_label_encoder: False  # Disable label encoder (deprecated in newer versions)
      tree_method: "hist"  # Use histogram-based algorithm for faster training
      random_state: ${system.random_seed}  # For reproducibility
    # Parameter ranges for hyperparameter optimization
    param_ranges:
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      max_depth: [3, 4, 5, 6, 8, 10]
      min_child_weight: [1, 3, 5, 7]
      subsample: [0.6, 0.7, 0.8, 0.9]
      colsample_bytree: [0.6, 0.7, 0.8, 0.9]
      n_estimators: [100, 200, 300, 500]

  # LightGBM configuration
  lightgbm:
    hyperparameters:
      learning_rate: 0.05  # Learning rate
      num_leaves: 31  # Number of leaves in one tree
      max_depth: -1  # Maximum tree depth, -1 means no limit
      min_data_in_leaf: 20  # Minimum number of data in one leaf
      feature_fraction: 0.8  # LightGBM will randomly select a subset of features on each iteration
      bagging_fraction: 0.8  # Like feature_fraction, but for data
      bagging_freq: 5  # Frequency for bagging
      objective: "binary"  # Binary classification objective
      metric: "auc"  # Evaluation metric
      boosting_type: "gbdt"  # Gradient boosting decision tree
      verbosity: -1  # Less verbosity
      random_state: ${system.random_seed}  # For reproducibility
    # Parameter ranges for hyperparameter optimization
    param_ranges:
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      num_leaves: [20, 31, 50, 80, 100]
      max_depth: [-1, 5, 10, 15, 20]
      min_data_in_leaf: [10, 20, 30, 50]
      feature_fraction: [0.6, 0.7, 0.8, 0.9]
      bagging_fraction: [0.6, 0.7, 0.8, 0.9]
      bagging_freq: [0, 5, 10]
      n_estimators: [100, 200, 300, 500]

  # Ensemble model configuration
  ensemble:
    hyperparameters:
      base_models: ["xgboost", "lightgbm"]  # Models to include in ensemble
      weights: [0.5, 0.5]  # Weights for each model
      voting: "soft"  # Soft voting uses predicted probabilities
      random_state: ${system.random_seed}  # For reproducibility

# Training configuration
training:
  test_size: 0.2  # Fraction of data to use for testing
  random_state: ${system.random_seed}  # For reproducible train/test splits
  
  # Early stopping configuration to prevent overfitting
  early_stopping:
    enabled: True  # Whether to use early stopping
    rounds: 50  # Stop if no improvement after this many rounds
    validation_ratio: 0.2  # Fraction of training data to use as validation
  
  # Cross-validation configuration
  cross_validation:
    enabled: True  # Whether to use cross-validation
    n_splits: 5  # Number of folds
    strategy: "time_series_split"  # Time-based splitting for temporal data
    max_train_size: null  # Maximum size for training set, null means no limit
    test_size: null  # Size of test set, null means default
    gap: 0  # Gap between train and test sets
  
  # Model retraining configuration
  retraining:
    schedule: "every_2_days"  # How often to retrain
    min_data_points: 1000  # Minimum data points required for retraining
    compare_with_previous: True  # Whether to compare with previous model
    min_improvement_threshold: 0.01  # Minimum improvement to accept new model

# Evaluation configuration
evaluation:
  # Metrics for model evaluation
  metrics:
    primary: "auc"  # Primary metric for model selection
    secondary: ["brier_score", "precision", "recall", "f1"]  # Additional metrics
    thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Probability thresholds for metrics
  
  # Calibration for probability outputs
  calibration:
    enabled: True  # Whether to calibrate probabilities
    n_bins: 10  # Number of bins for calibration
    strategy: "isotonic"  # Calibration method
  
  # Feature importance analysis
  feature_importance:
    enabled: True  # Whether to calculate feature importance
    method: "permutation"  # Method for calculating importance
    n_repeats: 10  # Number of permutation repeats

# Hyperparameter tuning configuration
hyperparameter_tuning:
  enabled: True  # Whether to perform hyperparameter tuning
  method: "bayesian"  # Optimization method (bayesian, grid, random)
  n_iterations: 50  # Number of iterations for optimization
  cv_folds: 5  # Number of cross-validation folds
  cv_strategy: "time_series_split"  # CV strategy for temporal data
  scoring: "auc"  # Scoring metric for optimization
  random_state: ${system.random_seed}  # For reproducibility
  parallel_jobs: ${system.parallel_jobs}  # Number of parallel jobs
  
  # Early stopping for optimization
  early_stopping:
    enabled: True  # Whether to use early stopping
    patience: 10  # Number of iterations with no improvement before stopping
    min_delta: 0.001  # Minimum change to qualify as improvement
  
  # Bayesian optimization settings
  bayesian_optimization:
    n_trials: 50  # Number of trials
    timeout: 3600  # Timeout in seconds (1 hour)
    n_jobs: 1  # Number of parallel jobs
    show_progress_bar: True  # Whether to show progress
  
  # Grid search settings
  grid_search:
    exhaustive: False  # Whether to try all combinations
    n_points: 5  # Number of points to sample per dimension if not exhaustive
  
  # Random search settings
  random_search:
    n_iter: 20  # Number of parameter settings to try