{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook provides a comprehensive guide on evaluating ERCOT RTLMP spike prediction models. It covers various aspects of model evaluation, including performance metrics calculation, visualization, and comparison across different thresholds and time periods.\n",
    "\n",
    "**Learning Objectives:**\n",
    "*   Understand the importance of model evaluation in the ERCOT RTLMP spike prediction system.\n",
    "*   Learn how to calculate and interpret various performance metrics, including AUC, precision, recall, and Brier score.\n",
    "*   Visualize model performance using ROC curves, precision-recall curves, calibration curves, and confusion matrices.\n",
    "*   Analyze model performance across different price thresholds and time periods.\n",
    "*   Compare the performance of multiple models and identify the best model for different scenarios.\n",
    "*   Generate a comprehensive evaluation report for stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import pandas as pd  # version 2.0+\n",
    "import numpy as np  # version 1.24+\n",
    "import matplotlib.pyplot as plt  # version 3.7+\n",
    "import seaborn as sns  # version 0.12+\n",
    "import plotly.express as px  # version 5.14+\n",
    "import plotly.graph_objects as go  # version 5.14+\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, brier_score_loss  # version 1.2+\n",
    "from sklearn.model_selection import train_test_split  # version 1.2+\n",
    "import datetime  # Standard\n",
    "import pathlib  # Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal modules\n",
    "from src.backend.models.evaluation import ModelEvaluator, ThresholdOptimizer\n",
    "from src.backend.visualization.performance_plots import ModelPerformancePlotter\n",
    "from src.backend.backtesting.performance_metrics import BacktestingMetricsCalculator\n",
    "from src.backend.models.xgboost_model import XGBoostModel\n",
    "from src.backend.data.fetchers.ercot_api import ERCOTDataFetcher\n",
    "from src.backend.features.feature_pipeline import FeaturePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants and utility functions\n",
    "PRICE_THRESHOLDS = [50.0, 100.0, 200.0, 300.0]\n",
    "DEFAULT_NODE = 'HB_NORTH'\n",
    "EVALUATION_METRICS = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'brier_score']\n",
    "MODEL_REGISTRY_PATH = '../models/registry'\n",
    "\n",
    "def create_target_variables(rtlmp_df: pd.DataFrame, thresholds: List[float]) -> pd.DataFrame:\n",
    "    \"\"\"Creates binary target variables for different price thresholds\"\"\"\n",
    "    targets = pd.DataFrame(index=rtlmp_df.index)\n",
    "    for threshold in thresholds:\n",
    "        hourly_data = rtlmp_df.groupby(pd.Grouper(key='timestamp', freq='H'))['price'].max()\n",
    "        targets[f'spike_occurred_{threshold}'] = (hourly_data > threshold).astype(int)\n",
    "    return targets\n",
    "\n",
    "def load_and_prepare_data(start_date: str, end_date: str, node: str, thresholds: List[float]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Loads historical data and prepares features and targets for model evaluation\"\"\"\n",
    "    data_fetcher = ERCOTDataFetcher()\n",
    "    rtlmp_df = data_fetcher.fetch_historical_data(start_date=start_date, end_date=end_date, identifiers=[node])\n",
    "    grid_df = data_fetcher.fetch_historical_data(start_date=start_date, end_date=end_date, identifiers=[])\n",
    "    feature_pipeline = FeaturePipeline()\n",
    "    feature_pipeline.add_data_source('rtlmp_df', rtlmp_df)\n",
    "    feature_pipeline.add_data_source('grid_df', grid_df)\n",
    "    features = feature_pipeline.create_features()\n",
    "    targets = create_target_variables(rtlmp_df, thresholds)\n",
    "    return features, targets\n",
    "\n",
    "def split_evaluation_data(features: pd.DataFrame, targets: pd.DataFrame, test_size: float, random_state: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits data into training and evaluation sets\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def compare_thresholds(metrics_by_threshold: Dict[float, Dict[str, float]], metrics_to_compare: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Compares model performance across different price thresholds\"\"\"\n",
    "    comparison_data = []\n",
    "    for threshold, metrics in metrics_by_threshold.items():\n",
    "        threshold_metrics = {metric: metrics[metric] for metric in metrics_to_compare}\n",
    "        threshold_metrics['threshold'] = threshold\n",
    "        comparison_data.append(threshold_metrics)\n",
    "    comparison_df = pd.DataFrame(comparison_data).set_index('threshold')\n",
    "    return comparison_df\n",
    "\n",
    "def plot_feature_importance(model: XGBoostModel, top_n: int) -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plots feature importance from a trained model\"\"\"\n",
    "    importance = model.get_feature_importance()\n",
    "    top_features = sorted(importance, key=importance.get, reverse=True)[:top_n]\n",
    "    values = [importance[feature] for feature in top_features]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(top_features, values)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'Top {top_n} Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare data for model evaluation. This includes fetching historical RTLMP data, creating target variables for different price thresholds, and splitting the data into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-01-01'\n",
    "end_date = '2023-06-30'\n",
    "node = DEFAULT_NODE\n",
    "thresholds = PRICE_THRESHOLDS\n",
    "\n",
    "features, targets = load_and_prepare_data(start_date, end_date, node, thresholds)\n",
    "X_train, X_test, y_train, y_test = split_evaluation_data(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a single model with standard metrics such as AUC, precision, recall, and Brier score. This provides a baseline understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train an XGBoost model\n",
    "model = XGBoostModel(model_id='eval_model')\n",
    "# Train the model if it hasn't been trained yet\n",
    "if not model.is_trained():\n",
    "    model.train(X_train, y_train['spike_occurred_100.0'])\n",
    "\n",
    "# Initialize ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "metrics = evaluator.evaluate(model, X_test, y_test['spike_occurred_100.0'])\n",
    "\n",
    "# Display evaluation results\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model performance with various plots, including ROC curves, precision-recall curves, calibration curves, and confusion matrices. These visualizations provide insights into different aspects of model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ModelPerformancePlotter\n",
    "plotter = ModelPerformancePlotter()\n",
    "plotter.load_model_data(model_id='eval_model', y_true=y_test['spike_occurred_100.0'], y_prob=model.predict_proba(X_test), y_pred=model.predict(X_test))\n",
    "\n",
    "# Create ROC curve plot\n",
    "fig_roc, ax_roc = plotter.plot_roc_curve()\n",
    "plt.show()\n",
    "\n",
    "# Create precision-recall curve plot\n",
    "fig_pr, ax_pr = plotter.plot_precision_recall_curve()\n",
    "plt.show()\n",
    "\n",
    "# Create calibration curve plot\n",
    "fig_cal, ax_cal = plotter.plot_calibration_curve()\n",
    "plt.show()\n",
    "\n",
    "# Create confusion matrix plot\n",
    "fig_cm, ax_cm = plotter.plot_confusion_matrix()\n",
    "plt.show()\n",
    "\n",
    "# Create comprehensive performance dashboard\n",
    "fig_dashboard = plotter.create_performance_dashboard()\n",
    "fig_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze model performance across different price thresholds. This helps understand the model's sensitivity to varying spike definitions and identify the optimal threshold for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model across different thresholds\n",
    "evaluator = ModelEvaluator()\n",
    "metrics_by_threshold = evaluator.evaluate_by_threshold(model, X_test, y_test, PRICE_THRESHOLDS)\n",
    "\n",
    "# Compare metrics between thresholds\n",
    "metrics_to_compare = ['precision', 'recall', 'f1', 'auc']\n",
    "comparison_df = compare_thresholds(metrics_by_threshold, metrics_to_compare)\n",
    "print(comparison_df)\n",
    "\n",
    "# Initialize ThresholdOptimizer\n",
    "optimizer = ThresholdOptimizer()\n",
    "\n",
    "# Find optimal threshold for different metrics\n",
    "optimal_threshold = optimizer.optimize_for_model(model, X_test, y_test['spike_occurred_100.0'])\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "\n",
    "# Plot threshold optimization curves\n",
    "fig_opt, ax_opt = optimizer.plot_optimization_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze model performance over different time periods, such as by hour of day, day of week, or month. This helps identify patterns in model performance and potential areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance over time\n",
    "evaluator = ModelEvaluator()\n",
    "time_column = 'timestamp'\n",
    "time_grouping = 'month'\n",
    "temporal_metrics = evaluator.evaluate_over_time(model, features, targets['spike_occurred_100.0'], time_column, time_grouping)\n",
    "\n",
    "# Plot metrics by hour of day\n",
    "fig_temporal, ax_temporal = plt.subplots(figsize=(10, 6))\n",
    "ax_temporal.plot(temporal_metrics.index, temporal_metrics['precision'], label='Precision')\n",
    "ax_temporal.plot(temporal_metrics.index, temporal_metrics['recall'], label='Recall')\n",
    "ax_temporal.set_xlabel('Month')\n",
    "ax_temporal.set_ylabel('Metric Value')\n",
    "ax_temporal.set_title('Temporal Performance Analysis')\n",
    "ax_temporal.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance between multiple models. This helps identify the best model for different scenarios and understand the trade-offs between different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple models for comparison\n",
    "model1 = XGBoostModel(model_id='model1')\n",
    "model2 = XGBoostModel(model_id='model2')\n",
    "\n",
    "# Train models if they haven't been trained yet\n",
    "if not model1.is_trained():\n",
    "    model1.train(X_train, y_train['spike_occurred_100.0'])\n",
    "if not model2.is_trained():\n",
    "    model2.train(X_train, y_train['spike_occurred_100.0'])\n",
    "\n",
    "# Initialize ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Compare models using ModelEvaluator\n",
    "comparison_df = evaluator.compare_models([model1, model2], X_test, y_test['spike_occurred_100.0'])\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize model comparison with bar charts\n",
    "plotter = ModelPerformancePlotter()\n",
    "model_metrics = {}\n",
    "model_metrics['model1'] = evaluator.evaluate(model1, X_test, y_test['spike_occurred_100.0'])\n",
    "model_metrics['model2'] = evaluator.evaluate(model2, X_test, y_test['spike_occurred_100.0'])\n",
    "fig_compare, ax_compare = plotter.plot_metric_comparison(model_metrics)\n",
    "plt.show()\n",
    "\n",
    "# Compare models across different thresholds\n",
    "metrics_by_threshold_model1 = evaluator.evaluate_by_threshold(model1, X_test, y_test, PRICE_THRESHOLDS)\n",
    "metrics_by_threshold_model2 = evaluator.evaluate_by_threshold(model2, X_test, y_test, PRICE_THRESHOLDS)\n",
    "\n",
    "# Identify the best model for different scenarios\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "for model_id, metrics_by_threshold in {'model1': metrics_by_threshold_model1, 'model2': metrics_by_threshold_model2}.items():\n",
    "    for threshold, metrics in metrics_by_threshold.items():\n",
    "        if metrics['auc'] > best_auc:\n",
    "            best_auc = metrics['auc']\n",
    "            best_model = model_id\n",
    "\n",
    "print(f\"Best model based on AUC: {best_model} with AUC = {best_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model performance through backtesting, simulating historical forecasts over a user-specified time window. This provides a more realistic assessment of model performance in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BacktestingMetricsCalculator\n",
    "calculator = BacktestingMetricsCalculator()\n",
    "\n",
    "# Run backtesting for a specific time window\n",
    "backtest_start = '2023-01-01'\n",
    "backtest_end = '2023-01-31'\n",
    "backtest_predictions, backtest_actuals = load_and_prepare_data(backtest_start, backtest_end, node, thresholds)\n",
    "backtesting_metrics = calculator.calculate_all_metrics(backtest_predictions, backtest_actuals, model_id='eval_model', thresholds=thresholds)\n",
    "\n",
    "# Visualize backtesting results\n",
    "fig_backtest, ax_backtest = plt.subplots(figsize=(10, 6))\n",
    "for threshold, metrics in backtesting_metrics.items():\n",
    "    ax_backtest.plot(metrics.keys(), metrics.values(), label=f'Threshold {threshold}')\n",
    "ax_backtest.set_xlabel('Metric')\n",
    "ax_backtest.set_ylabel('Value')\n",
    "ax_backtest.set_title('Backtesting Metrics')\n",
    "ax_backtest.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate comprehensive metrics from backtesting\n",
    "backtesting_report = calculator.generate_report(model_id='eval_model', output_path='backtest_report.json')\n",
    "print(backtesting_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze feature importance and impact on model performance. This helps identify key drivers of model performance and potential areas for feature engineering improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the model\n",
    "fig_importance, ax_importance = plot_feature_importance(model, top_n=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a complete evaluation report for stakeholders, summarizing key findings from the evaluation process. This report should include performance metrics, visualizations, and insights into model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report using ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "report = evaluator.generate_report(model, X_test, y_test['spike_occurred_100.0'], output_path='evaluation_report.json')\n",
    "\n",
    "# Format and display the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize key findings from the evaluation, discuss model strengths and weaknesses, and suggest improvements and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provided a comprehensive guide on evaluating ERCOT RTLMP spike prediction models. It covered various aspects of model evaluation, including performance metrics calculation, visualization, and comparison across different thresholds and time periods. By following the steps outlined in this notebook, you can gain a deep understanding of your model's performance and identify areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbreg": {
   "checksum": "sha256:99149011999999999999999999999999999"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}