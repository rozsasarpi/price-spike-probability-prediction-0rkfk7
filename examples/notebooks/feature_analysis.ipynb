# examples/notebooks/feature_analysis.ipynb
```python
#!/usr/bin/env python
# coding: utf-8

# # Introduction
# 
# This notebook provides a comprehensive analysis of the features used in the ERCOT RTLMP spike prediction system. It covers feature engineering, visualization, importance analysis, correlation analysis, and feature selection techniques.
# 
# ## Learning Objectives
# 
# - Understand the different types of features used in the model
# - Visualize feature distributions and correlations
# - Analyze feature importance using a trained model
# - Explore different feature selection methods
# - Evaluate the impact of feature selection on model performance

# # Setup and Imports

# In[1]:


# External libraries
import pandas as pd  # version 2.0+
import numpy as np  # version 1.24+
import matplotlib.pyplot as plt  # version 3.7+
import seaborn as sns  # version 0.12+
import plotly.express as px  # version 5.14+
import plotly.graph_objects as go  # version 5.14+
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.preprocessing import MinMaxScaler
import datetime  # Standard


# In[2]:


# Internal modules
from src.backend.data.fetchers.ercot_api import ERCOTDataFetcher
from src.backend.features.feature_pipeline import FeaturePipeline, DEFAULT_FEATURE_CONFIG
from src.backend.features.feature_registry import FeatureRegistry
from src.backend.models.xgboost_model import XGBoostModel
from src.backend.visualization.feature_importance import FeatureImportancePlotter


# In[3]:


# Global constants
PRICE_SPIKE_THRESHOLDS = [50.0, 100.0, 200.0, 300.0]
DEFAULT_NODES = ['HB_NORTH', 'HB_SOUTH', 'HB_WEST', 'HB_HOUSTON']
FEATURE_GROUPS = ['time', 'statistical', 'weather', 'market']
DATE_RANGE = ('2022-01-01', '2022-12-31')


# In[4]:


def load_and_prepare_data(start_date: str, end_date: str, nodes: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Loads historical data and prepares features for analysis
    
    Args:
        start_date: Start date for historical data
        end_date: End date for historical data
        nodes: List of node locations
        
    Returns:
        Tuple of (rtlmp_df, grid_df, features_df)
    """
    # Initialize ERCOTDataFetcher
    data_fetcher = ERCOTDataFetcher()
    
    # Fetch historical RTLMP data for the specified nodes and date range
    rtlmp_df = data_fetcher.fetch_historical_data(start_date=start_date, end_date=end_date, identifiers=nodes)
    
    # Fetch grid condition data for the same date range
    grid_df = data_fetcher.fetch_historical_data(start_date=start_date, end_date=end_date, identifiers=[])
    
    # Initialize FeaturePipeline with default configuration
    feature_pipeline = FeaturePipeline(feature_config=DEFAULT_FEATURE_CONFIG)
    
    # Add rtlmp_df and grid_df as data sources to the pipeline
    feature_pipeline.add_data_source(source_name='rtlmp_df', df=rtlmp_df)
    feature_pipeline.add_data_source(source_name='grid_df', df=grid_df)
    
    # Create features using the pipeline
    features_df = feature_pipeline.create_features()
    
    return rtlmp_df, grid_df, features_df


# # Data Loading and Feature Engineering

# In[5]:


# Load data and generate features for analysis
rtlmp_df, grid_df, features_df = load_and_prepare_data(start_date=DATE_RANGE[0], end_date=DATE_RANGE[1], nodes=DEFAULT_NODES)


# # Feature Overview

# In[6]:


# Initialize FeatureRegistry
feature_registry = FeatureRegistry()

# Retrieve all features from the registry
all_features = feature_registry.get_all_features()

# Display feature metadata by group
for group in FEATURE_GROUPS:
    features = feature_registry.get_features_by_group(group)
    print(f"Group: {group}")
    for feature_id, metadata in features.items():
        print(f"  - {feature_id}: {metadata['name']}")


# # Feature Distribution Analysis

# In[7]:


def analyze_feature_distributions(features_df: pd.DataFrame, feature_names: Optional[List[str]] = None) -> Dict[str, Dict[str, float]]:
    """
    Analyzes and visualizes the distributions of features
    
    Args:
        features_df: DataFrame containing features
        feature_names: List of feature names to analyze (optional)
        
    Returns:
        Dictionary with distribution statistics for each feature
    """
    # If feature_names is None, use all columns in features_df
    if feature_names is None:
        feature_names = features_df.columns.tolist()
    
    # Initialize empty dictionary to store distribution statistics
    distribution_stats = {}
    
    # For each feature, calculate basic statistics (mean, median, std, min, max, skew)
    for feature in feature_names:
        stats = {
            'mean': features_df[feature].mean(),
            'median': features_df[feature].median(),
            'std': features_df[feature].std(),
            'min': features_df[feature].min(),
            'max': features_df[feature].max(),
            'skew': features_df[feature].skew()
        }
        distribution_stats[feature] = stats
        
        # Create histograms for each feature
        plt.figure(figsize=(8, 6))
        sns.histplot(features_df[feature], kde=True)
        plt.title(f'Distribution of {feature}')
        plt.xlabel(feature)
        plt.ylabel('Frequency')
        plt.show()
        
        # Create box plots for each feature
        plt.figure(figsize=(6, 4))
        sns.boxplot(x=features_df[feature])
        plt.title(f'Box Plot of {feature}')
        plt.xlabel(feature)
        plt.show()
    
    return distribution_stats


# In[8]:


# Analyze distributions of all features
distribution_stats = analyze_feature_distributions(features_df)


# # Feature Correlation Analysis

# In[9]:


def analyze_feature_correlations(features_df: pd.DataFrame, feature_names: Optional[List[str]] = None, correlation_threshold: Optional[float] = None) -> pd.DataFrame:
    """
    Analyzes and visualizes correlations between features
    
    Args:
        features_df: DataFrame containing features
        feature_names: List of feature names to analyze (optional)
        correlation_threshold: Threshold for identifying highly correlated feature pairs (optional)
        
    Returns:
        Correlation matrix for the selected features
    """
    # If feature_names is None, use all columns in features_df
    if feature_names is None:
        feature_names = features_df.columns.tolist()
    
    # If correlation_threshold is None, use 0.7 as default
    if correlation_threshold is None:
        correlation_threshold = 0.7
    
    # Calculate correlation matrix for the selected features
    correlation_matrix = features_df[feature_names].corr()
    
    # Create heatmap visualization of the correlation matrix
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Feature Correlation Heatmap')
    plt.show()
    
    # Identify highly correlated feature pairs (above threshold)
    highly_correlated = set()
    for i in range(len(correlation_matrix.columns)):
        for j in range(i):
            if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
                colname_i = correlation_matrix.columns[i]
                colname_j = correlation_matrix.columns[j]
                highly_correlated.add((colname_i, colname_j))
    
    print("Highly Correlated Feature Pairs:")
    for pair in highly_correlated:
        print(pair)
    
    return correlation_matrix


# In[10]:


# Analyze correlations between features
correlation_matrix = analyze_feature_correlations(features_df)


# # Feature Importance Analysis

# In[11]:


def analyze_feature_importance(features_df: pd.DataFrame, target: pd.Series) -> Dict[str, float]:
    """
    Analyzes feature importance using a trained model
    
    Args:
        features_df: DataFrame containing features
        target: Series containing target variable
        
    Returns:
        Dictionary mapping feature names to importance scores
    """
    # Initialize and train an XGBoostModel on the features and target
    model = XGBoostModel(model_id='temp_model')
    model.train(features=features_df, targets=target)
    
    # Extract feature importance from the trained model
    importance_scores = model.get_feature_importance()
    
    # Initialize FeatureImportancePlotter
    plotter = FeatureImportancePlotter()
    
    # Create visualizations of feature importance
    fig_bar, ax_bar = plotter.plot_feature_importance(feature_importance=importance_scores)
    plt.show()
    
    fig_pie, ax_pie = plotter.plot_feature_group_importance(feature_importance=importance_scores)
    plt.show()
    
    # Analyze importance by feature group
    # (Implementation depends on how feature groups are defined)
    
    return importance_scores


# In[12]:


# Create binary target variable for price spikes
target = (rtlmp_df['price'] > 100).astype(int)

# Analyze feature importance
importance_scores = analyze_feature_importance(features_df, target)


# # Feature Group Analysis

# In[13]:


def analyze_feature_by_group(features_df: pd.DataFrame, feature_registry: FeatureRegistry) -> Dict[str, List[str]]:
    """
    Analyzes features by their group categories
    
    Args:
        features_df: DataFrame containing features
        feature_registry: FeatureRegistry instance
        
    Returns:
        Dictionary mapping feature groups to feature lists
    """
    # Initialize empty dictionary to store features by group
    features_by_group = {}
    
    # For each feature group in FEATURE_GROUPS:
    for group in FEATURE_GROUPS:
        # Get features belonging to that group from feature_registry
        features = feature_registry.get_features_by_group(group)
        
        # Filter features_df to include only features in that group
        group_features = [f for f in features_df.columns if f in features]
        group_df = features_df[group_features]
        
        # Analyze characteristics of features in the group
        print(f"Analyzing {group} features:")
        print(group_df.describe())
        
        # Create visualizations comparing feature groups
        # (Implementation depends on specific visualization needs)
        
        features_by_group[group] = group_features
    
    return features_by_group


# In[14]:


# Analyze features by their group categories
features_by_group = analyze_feature_by_group(features_df, feature_registry)


# # Feature Selection Techniques

# In[15]:


def perform_feature_selection(features_df: pd.DataFrame, target: pd.Series, method: str, n_features: Optional[int] = None) -> List[str]:
    """
    Performs feature selection using various techniques
    
    Args:
        features_df: DataFrame containing features
        target: Series containing target variable
        method: Feature selection method ('importance', 'correlation', 'recursive', 'statistical')
        n_features: Number of features to select (optional)
        
    Returns:
        List of selected feature names
    """
    # If n_features is None, use 20 as default
    if n_features is None:
        n_features = 20
    
    if method == 'importance':
        # Use feature importance from a trained model to select top features
        model = XGBoostModel(model_id='temp_model')
        model.train(features=features_df, targets=target)
        importance_scores = model.get_feature_importance()
        selected_features = sorted(importance_scores, key=importance_scores.get, reverse=True)[:n_features]
    elif method == 'correlation':
        # Use correlation-based feature selection to remove redundant features
        corr_matrix = features_df.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]
        selected_features = [feature for feature in features_df.columns if feature not in to_drop]
    elif method == 'recursive':
        # Use recursive feature elimination to select features
        model = XGBoostModel(model_id='temp_model')
        model.train(features=features_df, targets=target)
        selector = RFE(model.model, n_features_to_select=n_features, step=1)
        selector = selector.fit(features_df, target)
        selected_features = features_df.columns[selector.support_].tolist()
    elif method == 'statistical':
        # Use statistical tests to select features
        selector = SelectKBest(score_func=mutual_info_classif, k=n_features)
        selector.fit(features_df, target)
        selected_features = features_df.columns[selector.get_support()].tolist()
    else:
        raise ValueError(f"Invalid feature selection method: {method}")
    
    # Create visualizations comparing original and selected feature sets
    # (Implementation depends on specific visualization needs)
    
    return selected_features


# In[16]:


# Perform importance-based feature selection
selected_features_importance = perform_feature_selection(features_df, target, method='importance')

# Perform correlation-based feature selection
selected_features_correlation = perform_feature_selection(features_df, target, method='correlation')

# Perform recursive feature elimination
selected_features_recursive = perform_feature_selection(features_df, target, method='recursive')

# Perform statistical feature selection
selected_features_statistical = perform_feature_selection(features_df, target, method='statistical')


# # Temporal Pattern Analysis

# In[17]:


def analyze_temporal_patterns(features_df: pd.DataFrame, timestamp_column: str, feature_names: List[str]) -> Dict[str, Dict[str, pd.Series]]:
    """
    Analyzes temporal patterns in features
    
    Args:
        features_df: DataFrame containing features
        timestamp_column: Name of the timestamp column
        feature_names: List of feature names to analyze
        
    Returns:
        Dictionary with temporal analysis results
    """
    # Ensure timestamp_column exists in features_df
    if timestamp_column not in features_df.columns:
        raise ValueError(f"Timestamp column '{timestamp_column}' not found in features_df")
    
    # Initialize empty dictionary to store temporal analysis results
    temporal_analysis_results = {}
    
    # For each feature in feature_names:
    for feature in feature_names:
        # Analyze hourly patterns (hour of day)
        hourly_patterns = features_df.groupby(features_df[timestamp_column].dt.hour)[feature].mean()
        
        # Analyze daily patterns (day of week)
        daily_patterns = features_df.groupby(features_df[timestamp_column].dt.dayofweek)[feature].mean()
        
        # Analyze monthly patterns (month of year)
        monthly_patterns = features_df.groupby(features_df[timestamp_column].dt.month)[feature].mean()
        
        # Analyze seasonal patterns
        seasonal_patterns = features_df.groupby(features_df[timestamp_column].dt.quarter)[feature].mean()
        
        # Create time series visualizations
        plt.figure(figsize=(10, 6))
        hourly_patterns.plot(kind='line', title=f'Hourly Patterns of {feature}')
        plt.xlabel('Hour of Day')
        plt.ylabel(feature)
        plt.show()
        
        # Store temporal analysis results
        temporal_analysis_results[feature] = {
            'hourly': hourly_patterns,
            'daily': daily_patterns,
            'monthly': monthly_patterns,
            'seasonal': seasonal_patterns
        }
    
    return temporal_analysis_results


# In[18]:


# Analyze temporal patterns in features
temporal_analysis_results = analyze_temporal_patterns(features_df, 'timestamp', features_df.columns[:5].tolist())


# # Feature Engineering Recommendations

# Based on the analysis performed in this notebook, here are some recommendations for feature engineering improvements:
# 
# - **Optimal Feature Set:** The optimal feature set depends on the specific model and evaluation metric used. However, based on the feature importance analysis, the most important features are likely to be related to rolling statistics of RTLMP values, time-based features, and weather-related features.
# - **New Features to Develop:** Consider developing new features related to:
#     - Lagged RTLMP values
#     - Interactions between weather and market features
#     - More sophisticated measures of price volatility
# - **Feature Engineering Best Practices:**
#     - Always validate feature distributions and correlations to identify potential issues
#     - Use feature selection techniques to reduce dimensionality and improve model performance
#     - Continuously monitor feature importance and update the feature set as needed

# # Conclusion
# 
# This notebook provided a comprehensive analysis of the features used in the ERCOT RTLMP spike prediction system. By visualizing feature distributions, analyzing feature importance, and exploring different feature selection methods, we gained valuable insights into the predictive power of different features. These insights can be used to improve the feature engineering process and ultimately build more accurate and reliable models.
# 
# ## Next Steps
# 
# - Implement the feature engineering recommendations outlined above
# - Evaluate the impact of the new features on model performance
# - Continuously monitor feature importance and update the feature set as needed
# 
# ## References
# 
# - [ERCOT Real-Time Market Data](https://www.ercot.com/mktinfo/rtm)
# - [Scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)